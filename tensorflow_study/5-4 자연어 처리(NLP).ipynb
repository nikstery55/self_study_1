{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c17a7fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 인덱스 {'영실이는': 1, '정말': 2, '좋아해': 3, '나를': 4, '영화를': 5}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "자연어 처리(NLP) 스팸, 뉴스/쇼핑 카테고리 분류, 텍스트 요약, 문장 생성, 기계 번역,\n",
    "챗봇 등 다양한 분야에 활용되고 있다\n",
    "\n",
    "-자연어 처리 방법\n",
    "텍스트 데이터는 테이블 데이터와 같이 구조화되거나 데이터의 길이가 일정하지 않다는\n",
    "특성을 갖는다\n",
    "따라서 문장의 길이가 다를 경우 딥러닝 모델에 입력으로 넣기 위해서는 길이를 동일하게\n",
    "맞추는 작업이 선행되어야 한다 \n",
    "가장 긴 문장의 길이에 맞출 수도 있고, 가장 짧은 문장 길이에 맞춰 넘치는 부분을 \n",
    "잘라 낼 수도 있다\n",
    "\n",
    "또한 한글을 입력 데이터로 활용하기 위해서는 숫자로 변환해야 하고, 한글과 숫자를 일대일\n",
    "매칭하는 방식으로 단어 사전을 만든다 \n",
    "여기서 말하는 사전은 \"텐서플로\" : \"2021\", \"딥러닝\" : \"2022\" 와 같이 \n",
    "모든 단어 (토큰)를 숫자로 매핑한 사전이다\n",
    "사전을 통해 단어를 숫자로, 숫자를 단어로 변경할 수 있다\n",
    "\n",
    "이렇게 단어(토큰)를 숫자로 변경하기 위해서는 문장을 특정한 기준으로 잘라내야 하는데,\n",
    "이렇게 잘라낸 조각을 토큰이라고 표현한다 \n",
    "영어에서는 띄어쓰기를 기준으로 잘라내더라도 큰 문제가 없으나 \n",
    "한글에서는 띄어쓰기가 잘 되어 있지 않고 '~이(가)', '~을(를)', '~에게' 등\n",
    "조사(명사에 붙어서 다른 말과의 관계를 나타내거나 특별한 뜻을 더해 주는 품사)가 붙어 있어\n",
    "잘라내는 것에 어려움이 있다\n",
    "\n",
    "예를 들어 \"텐서플로가\", \"텐서플로를\", \"텐서플로는\"이 있다면, 컴퓨터는 3가지 모두 다른\n",
    "단어로 인식한다. 이에 띄어쓰기는 되어있지 않지만 \"텐서플로\"를 구분하는 토크나이저를 \n",
    "활용하거나 불용어(stopword) 처리를 통해 \"가\", \"를\", \"는\" 등 조사나 반복되는 불필요한 \n",
    "단어를 제거해야한다 \n",
    "\n",
    "다음 예제에서는 문장 텍스트 데이터를 처리해 딥러닝 모델에 입력으로 넣기 직전의 \n",
    "데이터셋을 준비하는 방법을 알아본다\n",
    "- 토큰화 ( 문장을 띄어쓰기 기준으로 나눔) + 단어 사전(단어와 숫자 매칭)\n",
    "- 문자 인코딩(사전을 바탕으로 문장들을 숫자로 변경)\n",
    "- 인코딩된 문장 길이를 동일하게 변경(패딩)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "텐서플로에서 제공하는 Tokenizer는 띄어쓰기를 기준으로 단어 인코딩 사전을 생성하고,\n",
    "단어를 쉽게 인코딩할 수 있게 도와준다 \n",
    "앞에서 그림으로 설명한 문장을 실제 케라스를 활용해서 단어 사전으로 변환한다\n",
    "Tokenizer 객체를 생성하고, fit_on_texts() 함수에 인코딩할 문장들을 입력한 뒤 \n",
    "단어 토큰을 만들고 각각 인덱스를 지정한다 \n",
    "\"\"\"\n",
    "#텐서플로 토크나이저\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "sentences = [ \"영실이는 나를 정말 정말 좋아해\", \n",
    "            \"영실이는 영화를 좋아해\"]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "print(\"단어 인덱스\", tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b9724e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4, 2, 2, 3], [1, 5, 3]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "texts_to_sequences() 함수는 입력된 문장을 단어 인덱스를 사용하여 숫자 벡터로 변환\n",
    "\"\"\"\n",
    "# 인코딩된 결과\n",
    "word_encoding = tokenizer.texts_to_sequences(sentences)\n",
    "word_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a30081b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4, 3]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "만약 사전에 없는 새로운 단어가 등장하면 새로운 단어는 인터딩할 때 무시한다\n",
    "다음 코드를 보면 \"경록이와\" 라는 단어는 앞에서 만든 사전에 없으므로 인코딩된 \n",
    "숫자에서 빠져있음을 확인할 수 있다\n",
    "\"\"\"\n",
    "# 사전에 없는 단어가 있을 때 인코딩 결과\n",
    "new_sentences = [\"영실이는 경록이와 나를 좋아해\"]\n",
    "new_word_encoding = tokenizer.texts_to_sequences(new_sentences)\n",
    "new_word_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e21e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, '영실이는': 2, '정말': 3, '좋아해': 4, '나를': 5, '영화를': 6}\n",
      "[[2, 1, 5, 4]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "앞의 코드와 같이,  자연어에는 사전에 없는 새로운 단어가 등장할 수 있다\n",
    "사전에 존재하지 않는 단어를 OOV(Out of Vocabulary) token이라고 부른다 \n",
    "케라스 Tokenizer는 이를 처리하기 위해 oov_token 파라미터 값을 설정할 수 있다\n",
    "\"\"\"\n",
    "# 사전에 없는 (Out of Vocabulary) 단어 처리 - oov : 경록이와 \n",
    "tokenizer = Tokenizer(oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "new_word_encoding = tokenizer.texts_to_sequences(new_sentences)\n",
    "\n",
    "print(word_index)\n",
    "print(new_word_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6acda788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, '영실이는': 2, '정말': 3, '좋아해': 4, '나를': 5, '영화를': 6}\n",
      "[[2, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "텍스트 데이터셋에 빈도수가 작은 단어가 많이 존재하는 경우에는 이들 단어를 제외하는\n",
    "것이 일반적이다 \n",
    "즉 문장을 토큰으로 인코딩할 때 빈도수가 많은 순서대로 최대 사전 개수를 정하고\n",
    "빈도수가 적은 단어를 제외한다\n",
    "최대 사전 개수는 num_words 파라미터를 통해 설정한다\n",
    "3으로 설정하는 경우 빈도수순으로 3개 토큰만 인코딩하고, 나머지 단어 \"1\"(OOV)로 \n",
    "인코딩될 것이다\n",
    "\"\"\"\n",
    "# 단어 사전 개수 설정\n",
    "tokenizer = Tokenizer(num_words=3, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "new_word_encoding = tokenizer.texts_to_sequences(new_sentences)\n",
    "\n",
    "print(word_index)\n",
    "print(new_word_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc18c5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4 2 2 3]\n",
      " [0 0 1 5 3]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "순환신경망(RNN)에 데이터를 입력으로 넣기 위해서는 문장의 길이(크기)를 동일하게 \n",
    "맞춰야 한다 \n",
    "이를 패딩(padding)이라고 부른다\n",
    "\n",
    "케라스에서 제공하는 pad_sequences() 함수를 활용하면 인코딩된 문자의 길이를 동일하게\n",
    "만들수 있다. 최대 문장의 길이를 기준으로 그보다 짧다면 앞에 0 값이 채워지는 것을 \n",
    "확인할 수 있다\n",
    "\n",
    "\"\"\"\n",
    "# 문장의 길이 맞추기\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded = pad_sequences(word_encoding)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56405002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4 2 2 3]\n",
      " [1 5 3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "한편, 뒤쪽에 0을 채우기 위해서는 padding 파라미터 값을 \"post\"로 설정한다\n",
    "값을 출력해보면 뒤쪽으로 부족한 개수만큼 0으로 채워지는 것을 확인할 수 있다\n",
    "\"\"\"\n",
    "#패딩(뒤에 0 붙이기)\n",
    "padded = pad_sequences(word_encoding, padding=\"post\")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5152e9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2 2 3]\n",
      " [1 5 3 0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "만약 몇몇 문장만 길이가 길고 대부분의 문장 길이(단어 개수)가 4 이하라면 최대값을 \n",
    "4로 설정 할 수도 있다\n",
    "\"\"\"\n",
    "# 문장의 최대 길이 고정\n",
    "padded = pad_sequences(word_encoding, padding=\"post\", maxlen=4)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02857dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4 2 2]\n",
      " [1 5 3 0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "이때 최대 길이인 4보다 긴 문장의 경우 잘라내야 하며, padding 과 동일하게 기본값은 \n",
    "앞부분이 잘리게 된다, 뒷부분을 자르고 싶을 때는 truncating 파라미터 값을 \"post\"로 \n",
    "설정하면 된다\n",
    "\"\"\"\n",
    "# 최대 길이보다 문장이 길 때 뒷부분 자르기\n",
    "padded = pad_sequences(word_encoding, padding=\"post\", truncating=\"post\", maxlen=4)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d623d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539183ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e2834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6cacb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66ea850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dae14b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5aa7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a9e310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b080a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158132fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
