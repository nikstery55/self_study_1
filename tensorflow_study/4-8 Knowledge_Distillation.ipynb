{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73aef59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Knowledge Distillation이란 \n",
    "파라미터 개수가 많은 큰 모델이 선생님이 되어 - 크기가 작은 모델을 가르치는 개념으로 Knowlage Distillation이라고 부른다\n",
    "- 직역하면 \"지식 증류\"\n",
    "큰 모델과 작은 모델의 예측의 오차(distillation loss)와 작은 모델의 손실함수(student loss)를 줄여 나가는 방향으로\n",
    "작은 모델의 파라미터를 최적화하게 된다\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e06efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ title 파라미터 설정\n",
    "t_ephoc = 5 # @param (type: \" slider\", min,1 max, 100, step1)\n",
    "s_ephoc = 10 # @param (type\"slider, min-1, max-100, step1)\n",
    "learning_rate = 0.01\n",
    "batch_size = 64 # @param [32, 64, 128, 256] [type\"raw\"]\n",
    "temperature = 3 # @param [type: \"slider\" min-1, max-10, step1]\n",
    "alpha = 0.5 # @param [type \"slider\" min- 0.1, max -0.9, step0.1]\"\\\n",
    "\n",
    "\"\"\"\n",
    "여기서는 안보임 colab으로 가야할듯 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c040e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
