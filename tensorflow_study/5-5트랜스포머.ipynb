{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba9e054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블: POSITIVE, score: 1.0\n",
      "레이블: POSITIVE, score: 1.0\n",
      "레이블: NEGATIVE, score: 1.0\n",
      "레이블: NEGATIVE, score: 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4개의 문장 중 앞 문장 두개는 긍정의 문장으로 구성하고, 뒤 문장 두개는 부정의 리뷰 글을\n",
    "임의로 작성한 것이다\n",
    "허깅페이스 파이프라인에서 감정분석을 나타내는 \"sentiment-analysis\" 모델을 불러온다\n",
    "실행 결과에서 당초 의도대로 긍정 레이블과 부정 레이블을 잘 분류하는 것을 알 수 있다\n",
    "\"\"\"\n",
    "# 감성분석\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "data = [\"This is what a true masterpiece looks like\", \n",
    "     \"brilliant film, hard to better\",\n",
    "     \"Are you kidding me. A horrible movie about horrible people.\",\n",
    "     \"the plot itself is also very boring\"]\n",
    "results = classifier(data)\n",
    "for result in results:\n",
    "    print(f\"레이블: {result['label']}, score: {round(result['score'], 3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247f3d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.6286845803260803}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "기본적으로 영어로 사전 훈련된 모델과 토크나이저를 활용한다. 다음 코드는 부정적인 글을\n",
    "한글로 작성했지만 결과는 \"긍정\"으로 나타났다\n",
    "모델이 문장을 제대로 이해하지 못했음을 알 수 있다\n",
    "\"\"\"\n",
    "classifier(\"나는 수학이 어렵다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956fd0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment/resolve/main/tf_model.h5\n",
      "404 Client Error: Not Found for url: https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment/resolve/main/tf_model.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model nlptown/bert-base-multilingual-uncased-sentiment with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification'>).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26536/3235178648.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \"\"\"\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# 다국어 모델 불러오기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m classifier = pipeline('sentiment-analysis', \n\u001b[0m\u001b[0;32m      9\u001b[0m                       model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n\u001b[0;32m     10\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"나는 수학이 어럽다\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensor1\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[1;31m# Will load the correct model if possible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[0mmodel_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m     framework, model = infer_framework_load_model(\n\u001b[0m\u001b[0;32m    544\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmodel_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensor1\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Could not load model {model} with any of the following classes: {class_tuple}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[0mframework\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"tf\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TF\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model nlptown/bert-base-multilingual-uncased-sentiment with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification'>)."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "이때 다국어로 학습한 모델을 불러온다면 결과는 달라진다.\n",
    "별 5점 만점 중 별 2점으로 부정적인 글로 인식했음을 확인할 수 있다\n",
    "\n",
    "링크가 없다고 뜸 - 뭐지\n",
    "\"\"\"\n",
    "# 다국어 모델 불러오기\n",
    "classifier = pipeline('sentiment-analysis', \n",
    "                      model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "classifier(\"나는 수학이 어럽다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "다국어 모델로 한글 문장을 더 확인해보자. 긍정적인 문장을 별 5점으로 \n",
    "부정인 문장은 별 3점과 2점으로 비교적 정확한 결과를 보인다\n",
    "링크에러로 - 실행안됨\n",
    "\"\"\"\n",
    "# 감성분석(다국어)\n",
    "classifier = pipeline('sentiment-analysis', \n",
    "                      model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "data = [\"이 영화 최고\", \n",
    "     \"너무 지루하다\",\n",
    "     \"또 보고싶은 최고의 걸작이다.\",\n",
    "     \"내 취향은 아니다.\"]\n",
    "\n",
    "results = classifier(data)\n",
    "\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"문장: {data[i]}, 레이블: {result['label']}, score: {round(result['score'], 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9cf666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16eff7d049ba4492ae07454c8fc46783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3479acda41848c1a0709bfd8b29eb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased-distilled-squad were not used when initializing TFDistilBertForQuestionAnswering: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased-distilled-squad and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e1e9c724dc4f41a0662e5abef50e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0ff05ad6a04181bf3101784aaa0726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828504e3f6194b6c87bbff3e254e4246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: What is TensorFlow?, 응답: 'Google Brain's second-generation system', score: 0.801\n",
      "질문: When is TensorFlow 2.0 announced?, 응답: 'Jan 2019', score: 0.771\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5-2 질의 응답\n",
    "사전에 학습된 질의응답(question-answering) 파이프 라인을 불러온다 \n",
    "위키피디아에서 tensorflow를 검색해서 나온 소개글을 데이터로 넣고 응답을 확인해보자\n",
    "입력된 텍스트 데이터에 대한 내용을 질문하면 문장 중에서 적절한 응답을 찾아서 제시한다\n",
    "이와 같은 질의 응답 문제는 지문이 나오고 그 지문과 관련된 복수의 질문을 물어보는\n",
    "국어/영어 시험 문제와 유사하다\n",
    "\"\"\"\n",
    "# 질의응답\n",
    "# https://en.wikipedia.org/wiki/TensorFlow 텐서플로 소개 글 중 일부\n",
    "from transformers import pipeline\n",
    "nlp = pipeline(\"question-answering\")\n",
    "data = r\"\"\"\n",
    "TensorFlow is Google Brain's second-generation system. Version 1.0.0 was released on February 11, 2017.[14] While the reference implementation runs on single devices, TensorFlow can run on multiple CPUs and GPUs (with optional CUDA and SYCL extensions for general-purpose computing on graphics processing units).[15] TensorFlow is available on 64-bit Linux, macOS, Windows, and mobile computing platforms including Android and iOS.\n",
    "Its flexible architecture allows for the easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices.\n",
    "TensorFlow computations are expressed as stateful dataflow graphs. The name TensorFlow derives from the operations that such neural networks perform on multidimensional data arrays, which are referred to as tensors. During the Google I/O Conference in June 2016, Jeff Dean stated that 1,500 repositories on GitHub mentioned TensorFlow, of which only 5 were from Google.[16]\n",
    "In December 2017, developers from Google, Cisco, RedHat, CoreOS, and CaiCloud introduced Kubeflow at a conference. Kubeflow allows operation and deployment of TensorFlow on Kubernetes.\n",
    "In March 2018, Google announced TensorFlow.js version 1.0 for machine learning in JavaScript.[17]\n",
    "In Jan 2019, Google announced TensorFlow 2.0.[18] It became officially available in Sep 2019.[19]\n",
    "In May 2019, Google announced TensorFlow Graphics for deep learning in computer graphics.[20]\n",
    "\"\"\"\n",
    "q1 = \"What is TensorFlow?\"\n",
    "result = nlp(question=q1, context=data)\n",
    "print(f\"질문: {q1}, 응답: '{result['answer']}', score: {round(result['score'], 3)}\")\n",
    "\n",
    "q2 = \"When is TensorFlow 2.0 announced?\"\n",
    "result = nlp(question=\"When is TensorFlow 2.0 announced?\", context=data)\n",
    "print(f\"질문: {q2}, 응답: '{result['answer']}', score: {round(result['score'], 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed15844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e887e9c58834774a6f53bf292d363b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259455acea3343e89ba5dfed4492edb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/475M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb4513f85f9424fb4293ef2ebc53ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c194ff6152a7468da465e75a26c1502a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2536b30b6a4aa9a3c1089e9e81f1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I love you, I will never forget you.'}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5-3 문장 생성\n",
    "문장 생성(text-generation)파이프라인을 불러온 뒤 시작(Seed) 문장으로 \"I love you, I will\"을\n",
    "입력하고, 최대 생성할 문장의 길이(10)를 지정한다 \n",
    "결과를 보면 누가 봐도 깔끔한 문장이 생성되었다\n",
    "\"\"\"\n",
    "# 문자 생성\n",
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "data = \"I love you, I will\"\n",
    "print(text_generator(data, max_length=10, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b13045c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-small (https://huggingface.co/t5-small)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c567d8f6c63547faa3d454cb26e338e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba27cb3b6bd04849b9d7493e6a0c2e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c190d141b447f08795d09415704e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5de782af6ea4ff2a1bf9d736b730fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'TensorFlow is available on 64-bit Linux, macOS, Windows, and mobile computing platforms including Android and iOS . its flexible architecture allows for the easy deployment of computation across a variety of platforms (CPUs, GPU'}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5-4 문장 요약\n",
    "위키피디아에 있는 텐서플로 소개글을 다시 활용한다 \n",
    "이 문장을 요약하는 문제다. 10~50개의 단어로 결과가 나올 수 있도록 설정하고 \n",
    "요약으로 출력된 문장을 보면, 텐서플로가 구글 브레인의 두 번째 시스템이며 \n",
    "언제 처음 출시되었는지, 어떤 시스템에 이용 가능한지 요약해서 설명하고 있다\n",
    "\"\"\"\n",
    "# 문장요약\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "data =\"\"\"\n",
    "TensorFlow is Google Brain's second-generation system. Version 1.0.0 was released on February 11, 2017.[14] While the reference implementation runs on single devices, TensorFlow can run on multiple CPUs and GPUs (with optional CUDA and SYCL extensions for general-purpose computing on graphics processing units).[15] TensorFlow is available on 64-bit Linux, macOS, Windows, and mobile computing platforms including Android and iOS.\n",
    "Its flexible architecture allows for the easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices.\n",
    "TensorFlow computations are expressed as stateful dataflow graphs. The name TensorFlow derives from the operations that such neural networks perform on multidimensional data arrays, which are referred to as tensors. During the Google I/O Conference in June 2016, Jeff Dean stated that 1,500 repositories on GitHub mentioned TensorFlow, of which only 5 were from Google.[16]\n",
    "In December 2017, developers from Google, Cisco, RedHat, CoreOS, and CaiCloud introduced Kubeflow at a conference. Kubeflow allows operation and deployment of TensorFlow on Kubernetes.\n",
    "In March 2018, Google announced TensorFlow.js version 1.0 for machine learning in JavaScript.[17]\n",
    "In Jan 2019, Google announced TensorFlow 2.0.[18] It became officially available in Sep 2019.[19]\n",
    "In May 2019, Google announced TensorFlow Graphics for deep learning in computer graphics.[20]\n",
    "\"\"\"\n",
    "\n",
    "print(summarizer(data, max_length=50, min_length=10, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "허깅페이스 문서를 참조하세요\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6180acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a49111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290b90f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
